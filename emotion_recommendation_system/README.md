# ê°ì • ë¶„ì„ ê¸°ë°˜ ë…¸ë˜ ì¶”ì²œ ì‹œìŠ¤í…œ

RoBERTaì™€ BERTë¥¼ í™œìš©í•œ í•œêµ­ì–´ ë…¸ë˜ ê°€ì‚¬ ê¸°ë°˜ ê°ì • ë¶„ì„ ë° ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸµ ì£¼ìš” ê¸°ëŠ¥

### 1. ê°ì • ë¶„ë¥˜ ëª¨ë¸
- **ëª¨ë¸**: KLUE-RoBERTa-base ê¸°ë°˜
- **ì¶œë ¥**: 6ê°œ ê°ì •êµ°ë³„ ë¹„ìœ¨ ì˜ˆì¸¡ (ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€, ë†€ë¼ì›€, í˜ì˜¤)
- **ì†ì‹¤ í•¨ìˆ˜**: KL Divergence, Jensen-Shannon Divergence, Earth Mover's Distance, Focal Loss, MSE ì§€ì›

### 2. ë¬¸ë§¥ ë²¡í„° ìƒì„±
- **ëª¨ë¸**: KLUE-BERT-base ë˜ëŠ” Sentence-BERT (ko-sroberta-multitask)
- **ê¸°ëŠ¥**: ê°€ì‚¬ì˜ ì˜ë¯¸ì  ë¬¸ë§¥ì„ ë²¡í„°ë¡œ ì¸ì½”ë”©

### 3. í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ
- **ë¬¸ë§¥ ìœ ì‚¬ë„**: Sentence-BERT ì„ë² ë”© ê¸°ë°˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- **ê°ì • ìœ ì‚¬ë„**: ê°ì • ë¶„í¬ ê°„ KL/JS Divergence ë˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- **FAISS ì¸ë±ìŠ¤**: ë¹ ë¥¸ ëŒ€ê·œëª¨ ê²€ìƒ‰ ì§€ì›

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
emotion_recommendation_system/
â”œâ”€â”€ data_processor.py          # ë°ì´í„° ì „ì²˜ë¦¬ ë° í† í°í™”
â”œâ”€â”€ emotion_model.py           # RoBERTa ê°ì • ë¶„ë¥˜ ëª¨ë¸
â”œâ”€â”€ context_encoder.py         # BERT ë¬¸ë§¥ ë²¡í„° ìƒì„±
â”œâ”€â”€ training_pipeline.py       # í›ˆë ¨ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ main.py                   # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ requirements.txt          # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ README.md                # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â””â”€â”€ demo.py                  # ê°„ë‹¨í•œ ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸
```

## ğŸ› ï¸ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 1. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 2. í•„ìˆ˜ íŒ¨í‚¤ì§€
- torch >= 1.9.0
- transformers >= 4.20.0
- sentence-transformers >= 2.2.0
- faiss-cpu >= 1.7.0
- scikit-learn >= 1.0.0
- pandas, numpy, matplotlib, seaborn

## ğŸš€ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ ì‹¤í–‰ (ìƒ˜í”Œ ë°ì´í„°)
```bash
python main.py --num_samples 1000 --num_epochs 5
```

### 2. ì»¤ìŠ¤í…€ ë°ì´í„° ì‚¬ìš©
```bash
python main.py --data_path your_data.csv --num_epochs 10
```

#### ë°ì´í„° í˜•ì‹ (CSV)
```csv
lyrics,joy,sadness,anger,fear,surprise,disgust
"ì‚¬ë‘í•˜ëŠ” ë§ˆìŒì´ ë„ˆë¬´ ì»¤ì„œ í–‰ë³µí•´ìš”",0.7,0.1,0.05,0.05,0.05,0.05
"ì´ë³„ì˜ ì•„í””ì´ ë„ˆë¬´ ìŠ¬í¼ìš”",0.1,0.7,0.05,0.05,0.05,0.05
```

### 3. ê³ ê¸‰ ì„¤ì •
```bash
python main.py \
  --model_name klue/roberta-base \
  --context_model_name klue/bert-base \
  --loss_type kl_divergence \
  --batch_size 16 \
  --learning_rate 2e-5 \
  --num_epochs 20 \
  --save_dir ./results \
  --use_wandb
```

### 4. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
```bash
python main.py \
  --checkpoint_path ./results/best_model.pt \
  --skip_training
```

## ğŸ“Š ì¶œë ¥ íŒŒì¼

í›ˆë ¨ ì™„ë£Œ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

- `config.json`: ëª¨ë¸ ì„¤ì •
- `best_model.pt`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
- `recommendation_engine.pkl`: ì™„ì „í•œ ì¶”ì²œ ì‹œìŠ¤í…œ
- `training_curves.png`: í›ˆë ¨ ê³¼ì • ì‹œê°í™”
- `emotion_analysis.png`: ê°ì • ë¶„í¬ ë¶„ì„
- `recommendation_evaluation.png`: ì¶”ì²œ ì„±ëŠ¥ í‰ê°€
- `demo_results.json`: ì¶”ì²œ ë°ëª¨ ê²°ê³¼
- `evaluation_results.json`: ì •ëŸ‰ì  í‰ê°€ ê²°ê³¼

## ğŸ¯ ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ

### KL Divergence (ê¸°ë³¸ê°’)
- í™•ë¥  ë¶„í¬ ê°„ ì°¨ì´ ì¸¡ì •
- ë¹„ëŒ€ì¹­ì , ì •ë³´ ì´ë¡  ê¸°ë°˜
- ê°ì • ë¶„í¬ ì˜ˆì¸¡ì— ì í•©

### Jensen-Shannon Divergence
- KL Divergenceì˜ ëŒ€ì¹­ì  ë²„ì „
- ë” ì•ˆì •ì ì¸ í›ˆë ¨
- 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”

### Earth Mover's Distance
- ê°ì • ê°„ ìˆœì„œì  ê´€ê³„ ê³ ë ¤
- ìœ ì‚¬í•œ ê°ì • ê°„ ê±°ë¦¬ ìµœì†Œí™”
- íšŒê·€ ë¬¸ì œì— íš¨ê³¼ì 

### Focal Loss
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°
- ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘
- í¬ê·€ ê°ì • ë¶„ë¥˜ í–¥ìƒ

## ğŸ” ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜

### 1. ë¬¸ë§¥ ê¸°ë°˜ ì¶”ì²œ
```python
# ê°€ì‚¬ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
recommendations = engine.recommend_by_lyrics(
    query_lyrics="ì‚¬ë‘í•˜ëŠ” ë§ˆìŒì´ ì»¤ìš”",
    predicted_emotion=emotion_vector,
    context_weight=1.0  # ë¬¸ë§¥ë§Œ ì‚¬ìš©
)
```

### 2. ê°ì • ê¸°ë°˜ ì¶”ì²œ
```python
# ê°ì • ë¶„í¬ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
recommendations = engine.recommend_by_emotion_only(
    query_emotion=emotion_vector,
    method='kl_divergence'
)
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ê¶Œì¥)
```python
# ë¬¸ë§¥ê³¼ ê°ì •ì„ ê²°í•©
recommendations = engine.recommend_by_lyrics(
    query_lyrics="ì‚¬ë‘í•˜ëŠ” ë§ˆìŒì´ ì»¤ìš”",
    predicted_emotion=emotion_vector,
    context_weight=0.7  # ë¬¸ë§¥ 70%, ê°ì • 30%
)
```

## ğŸ“ˆ ì„±ëŠ¥ í‰ê°€

ì‹œìŠ¤í…œì€ ë‹¤ìŒ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤:

### ê°ì • ë¶„ë¥˜ ì„±ëŠ¥
- **MSE/MAE**: ê°ì • ë¹„ìœ¨ ì˜ˆì¸¡ ì •í™•ë„
- **Pearson ìƒê´€ê³„ìˆ˜**: ì˜ˆì¸¡-ì‹¤ì œ ê°ì • ìƒê´€ê´€ê³„
- **KL/JS Divergence**: ë¶„í¬ ì°¨ì´ ì¸¡ì •

### ì¶”ì²œ ì„±ëŠ¥
- **ìœ ì‚¬ë„ ì ìˆ˜**: ìƒìœ„ ì¶”ì²œ í•­ëª©ë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
- **ë‹¤ì–‘ì„±**: ì¶”ì²œ ê²°ê³¼ì˜ ê°ì •ì  ë‹¤ì–‘ì„±
- **ì ì¤‘ë¥ **: ì‚¬ìš©ì ì„ í˜¸ì™€ì˜ ì¼ì¹˜ë„

## ğŸ® ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸

ê°„ë‹¨í•œ ëŒ€í™”í˜• ë°ëª¨ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´:

```bash
python demo.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì •ì„ ë¶„ì„í•˜ê³  ìœ ì‚¬í•œ ê³¡ë“¤ì„ ì¶”ì²œí•©ë‹ˆë‹¤.

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ ì¶”ê°€
`emotion_model.py`ì˜ `LossFunction` í´ë˜ìŠ¤ì— ìƒˆ ë©”ì„œë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.

### 2. ê°ì • ì¹´í…Œê³ ë¦¬ ë³€ê²½
ê¸°ë³¸ 6ê°œ ê°ì • ì™¸ì— ë‹¤ë¥¸ ê°ì • ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `DataProcessor`ì˜ `emotion_labels`ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.

### 3. ë‹¤ë¥¸ ì–¸ì–´ ì§€ì›
ëª¨ë¸ ì´ë¦„ì„ í•´ë‹¹ ì–¸ì–´ì˜ BERT/RoBERTa ëª¨ë¸ë¡œ ë³€ê²½í•˜ì„¸ìš”:
```python
--model_name bert-base-multilingual-cased
--context_model_name sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
```

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- [KLUE: Korean Language Understanding Evaluation](https://arxiv.org/abs/2105.09680)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ“ ë¬¸ì˜ì‚¬í•­

í”„ë¡œì íŠ¸ì— ëŒ€í•œ ë¬¸ì˜ë‚˜ ê°œì„  ì œì•ˆì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”.
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onewon1234/AI_DL_Project/blob/KLUE-BERT/KLUE-BERT/KlueBert_code_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkl_GWPWLGnC",
        "outputId": "b3c36835-ed23-4e36-9204-8b177dc21de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZ3B4tyLG9k",
        "outputId": "fbbf76b9-08a3-43cc-8eac-a4f51c199249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì••ì¶• í•´ì œ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "file_path = '/content/drive/MyDrive/data/open.zip'\n",
        "\n",
        "extract_path = '/content/data'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"ì••ì¶• í•´ì œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOiE1UagLIFA",
        "outputId": "37309896-e8f4-488f-8025-99e7a0a6dbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sample_submission.csv', 'test.csv', 'train.csv']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(os.listdir(extract_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6HoTp-HLLTb",
        "outputId": "e3f58a04-1888-46bc-80fe-8d540655f984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          ID  answer_0  answer_1  answer_2  answer_3\n",
            "0  TEST_0000         0         1         2         3\n",
            "1  TEST_0001         0         1         2         3\n",
            "2  TEST_0002         0         1         2         3\n",
            "3  TEST_0003         0         1         2         3\n",
            "4  TEST_0004         0         1         2         3\n",
            "           ID                                         sentence_0  \\\n",
            "0  TRAIN_0000                 ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì€ íˆ¬í‘œ ê³¼ì •ì˜ íˆ¬ëª…ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.   \n",
            "1  TRAIN_0001  ì¤„ê±°ë¦¬ ìë™ ìƒì„±ì˜ ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜ì€ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì •ë³´ë¥¼...   \n",
            "2  TRAIN_0002  ë§ˆì§€ë§‰ìœ¼ë¡œ, í‚¤ì¹œíƒ€ì˜¬ì„ ë³´ê´€í•  ë•ŒëŠ” ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ê³³ì— ë‘ì–´ ë‚­ë¹„ë¥¼ ë°©ì§€í•˜ëŠ”...   \n",
            "3  TRAIN_0003   ì±…ì˜ í˜ì´ì§€ê°€ ì†ìƒë˜ì§€ ì•Šë„ë¡ ìˆ˜ì§ìœ¼ë¡œ ì„¸ì›Œ ë‘ê±°ë‚˜ í‰í‰í•˜ê²Œ ëˆ•í˜€ ë³´ê´€í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.   \n",
            "4  TRAIN_0004  ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ë°˜ë³µì ì¸ ì‹¤í—˜ì„ í†µí•´ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ë©°, ì´ë¥¼ í†µí•´ ë°œê²¬ì˜ ì •í™•ì„±...   \n",
            "\n",
            "                                          sentence_1  \\\n",
            "0  ì´ëŸ¬í•œ íŠ¹ì„±ì€ ìœ ê¶Œìë“¤ì—ê²Œ ì‹ ë¢°ë¥¼ ì œê³µí•˜ë©°, ë¯¼ì£¼ì  ì°¸ì—¬ë¥¼ ì´‰ì§„í•˜ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.   \n",
            "1     ê²°ê³¼ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì‚¬ìš©ìì—ê²Œ ì‹ ì†í•˜ê³  íš¨ìœ¨ì ì¸ ì •ë³´ ì „ë‹¬ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.   \n",
            "2          ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì²œì´ë‚˜ ìŠ¤í€ì§€ë¥¼ í™œìš©í•˜ë©´ í‚¤ì¹œíƒ€ì˜¬ì˜ í•„ìš”ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.   \n",
            "3      ì •ê¸°ì ìœ¼ë¡œ ë¨¼ì§€ë¥¼ í„¸ì–´ë‚´ê³ , ê³°íŒ¡ì´ë‚˜ í•´ì¶©ì˜ ë°œìƒ ì—¬ë¶€ë¥¼ ì ê²€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.   \n",
            "4  ì¸ê³µì§€ëŠ¥ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ¨ê²¨ì§„ íŒ¨í„´ê³¼ ìƒê´€ê´€ê³„ë¥¼ ë°œê²¬í•˜ëŠ” ë° ê°•ë ¥í•œ ë„...   \n",
            "\n",
            "                                          sentence_2  \\\n",
            "0  ê²°ê³¼ì ìœ¼ë¡œ ë¸”ë¡ì²´ì¸ ê¸°ë°˜ì˜ íˆ¬í‘œ ì‹œìŠ¤í…œì€ ê³µì •í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„ ê±° í™˜ê²½ì„ ì¡°ì„±...   \n",
            "1     ìƒì„±ëœ ì¤„ê±°ë¦¬ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ê°„ê²°í•˜ê²Œ ìš”ì•½ëœ í˜•íƒœë¡œ ì œê³µëœë‹¤.   \n",
            "2               ë¬¼ê¸°ë¥¼ ì œê±°í•  ë•ŒëŠ” ê°€ë³ê²Œ ëˆŒëŸ¬ì£¼ì–´ ê³¼ë„í•œ ì‚¬ìš©ì„ í”¼í•  ìˆ˜ ìˆë‹¤.   \n",
            "3             ì¢…ì´ì±…ì€ ì§ì‚¬ê´‘ì„ ì´ ë‹¿ì§€ ì•ŠëŠ” ì„œëŠ˜í•˜ê³  ê±´ì¡°í•œ ì¥ì†Œì— ë³´ê´€í•´ì•¼ í•œë‹¤.   \n",
            "4  ê²°êµ­, ì¸ê³µì§€ëŠ¥ì˜ ì§€ì›ì€ ê³¼í•™ì  ë°œê²¬ì˜ ì†ë„ì™€ íš¨ìœ¨ì„±ì„ í˜ì‹ ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ”...   \n",
            "\n",
            "                                          sentence_3  answer_0  answer_1  \\\n",
            "0       ê° íˆ¬í‘œëŠ” ë³€ê²½ ë¶ˆê°€ëŠ¥í•œ ê¸°ë¡ìœ¼ë¡œ ì €ì¥ë˜ì–´ ì¡°ì‘ì˜ ê°€ëŠ¥ì„±ì„ ì›ì²œì ìœ¼ë¡œ ì°¨ë‹¨í•œë‹¤.         0         3   \n",
            "1  ì´ ì•Œê³ ë¦¬ì¦˜ì€ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•˜ê³ , ì£¼ìš” ì‚¬ê±´ê³¼ ë“±ì¥ì¸ë¬¼ì„...         0         3   \n",
            "2                 í‚¤ì¹œíƒ€ì˜¬ì„ ì ˆì•½í•˜ëŠ” ì²«ê±¸ìŒì€ í•„ìš”í•œ ì–‘ë§Œí¼ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.         3         2   \n",
            "3  í•„ìš”í•  ê²½ìš°, ì±…ì„ ë³´í˜¸í•˜ê¸° ìœ„í•´ ì»¤ë²„ë¥¼ ì”Œìš°ê±°ë‚˜ ì „ìš© ë³´ê´€í•¨ì— ë„£ëŠ” ë°©ë²•ë„ ê³ ë ¤í• ...         2         0   \n",
            "4  ì´ëŸ¬í•œ ë¶„ì„ ê²°ê³¼ëŠ” ì—°êµ¬ìë“¤ì—ê²Œ ìƒˆë¡œìš´ ê°€ì„¤ì„ ì œì‹œí•˜ê³  ì‹¤í—˜ ì„¤ê³„ë¥¼ ê°œì„ í•˜ëŠ” ë° ê¸°...         1         3   \n",
            "\n",
            "   answer_2  answer_3  \n",
            "0         1         2  \n",
            "1         2         1  \n",
            "2         1         0  \n",
            "3         1         3  \n",
            "4         0         2  \n",
            "          ID                                         sentence_0  \\\n",
            "0  TEST_0000  ììœ  ì˜ì§€ì™€ ê²°ì •ë¡ ì€ ì„œë¡œ ìƒì¶©í•˜ëŠ” ê°œë…ìœ¼ë¡œ ì—¬ê²¨ì§€ì§€ë§Œ, ì´ ë‘˜ì˜ ê³µì¡´ ê°€ëŠ¥ì„±ë„ íƒ...   \n",
            "1  TEST_0001                  ì‚¬íšŒì  ë‚™ì¸ì€ ê°œì¸ì˜ ìì•„ ì¡´ì¤‘ê°ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.   \n",
            "2  TEST_0002                    ê¸€ì“°ê¸° ëŠ¥ë ¥ì„ í‚¤ìš°ê¸° ìœ„í•´ì„œëŠ” ê¾¸ì¤€í•œ ì—°ìŠµì´ í•„ìˆ˜ì ì´ë‹¤.   \n",
            "3  TEST_0003             ì‘ì€ ê³µê°„ì—ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ì§‘ì•ˆì˜ í˜¼ì¡í•¨ì„ ì¤„ì—¬ì¤€ë‹¤.   \n",
            "4  TEST_0004                  ìŒì•…ì€ íŠ¹ì • ë¬¸í™”ì˜ ê°€ì¹˜ì™€ ì „í†µì„ ë°˜ì˜í•˜ëŠ” ì¤‘ìš”í•œ ë§¤ì²´ì´ë‹¤.   \n",
            "\n",
            "                                          sentence_1  \\\n",
            "0  ê²°ì •ë¡ ì€ ëª¨ë“  ì‚¬ê±´ì´ ì›ì¸ê³¼ ê²°ê³¼ì˜ ì—°ì‡„ì— ì˜í•´ ë°œìƒí•œë‹¤ê³  ì£¼ì¥í•˜ë©°, ì´ëŠ” ì¸ê°„ì˜ ...   \n",
            "1  ê±´ê°• ë¶ˆí‰ë“±ì€ ì´ëŸ¬í•œ ë‚™ì¸ìœ¼ë¡œ ì¸í•´ ë”ìš± ì‹¬í™”ë˜ë©°, íŠ¹ì • ì§‘ë‹¨ì´ ì˜ë£Œ ì„œë¹„ìŠ¤ ì ‘ê·¼ì—...   \n",
            "2  ë§ˆì§€ë§‰ìœ¼ë¡œ, ë…ì„œë¥¼ í†µí•´ ë‹¤ë¥¸ ì‘ê°€ë“¤ì˜ ê¸°ë²•ì„ ë°°ìš°ëŠ” ê²ƒì€ ì°½ì˜ë ¥ì„ ìê·¹í•˜ëŠ” ë° ë„...   \n",
            "3                 ì •ê¸°ì ìœ¼ë¡œ ë‚´ìš©ì„ ì ê²€í•˜ë©´ í•„ìš” ì—†ëŠ” ë¬¼ê±´ì„ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.   \n",
            "4                ì´ëŸ¬í•œ ìŒì•…ì  í‘œí˜„ì€ ê³µë™ì²´ì˜ ì†Œì†ê°ì„ ì¦ì§„ì‹œí‚¤ëŠ” ì—­í• ì„ í•œë‹¤.   \n",
            "\n",
            "                                          sentence_2  \\\n",
            "0  ê·¸ëŸ¬ë‚˜ ì¸ê°„ì˜ ì¸ì‹ê³¼ ì„ íƒ ê³¼ì •ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë³µì¡ì„±ê³¼ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±ì€ ììœ  ì˜ì§€ì˜ ...   \n",
            "1  ê²°êµ­, ì‚¬íšŒì  ë‚™ì¸ê³¼ ê±´ê°• ë¶ˆí‰ë“±ì€ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í¬ê´„...   \n",
            "2              í”¼ë“œë°±ì„ ë°›ëŠ” ê³¼ì •ì€ ê¸€ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ìš”ì†Œë¡œ ì‘ìš©í•œë‹¤.   \n",
            "3                  ê° ì¹¸ì„ í™œìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¬¼ê±´ì„ ë‚˜ëˆ„ë©´ ì°¾ê¸° ì‰¬ì›Œì§„ë‹¤.   \n",
            "4               ê° ë¬¸í™”ëŠ” ê³ ìœ í•œ ìŒì•…ì  ìš”ì†Œë¥¼ í†µí•´ ì •ì²´ì„±ì„ í˜•ì„±í•˜ê³  ê°•í™”í•œë‹¤.   \n",
            "\n",
            "                                          sentence_3  \n",
            "0  ê²°êµ­, ììœ  ì˜ì§€ì™€ ê²°ì •ë¡ ì€ ì„œë¡œë¥¼ ë°°ì œí•˜ê¸°ë³´ë‹¤ëŠ”, ì¸ê°„ ê²½í—˜ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ì„¤ëª…...  \n",
            "1  ë‚™ì¸ìœ¼ë¡œ ì¸í•´ ì‚¬ëŒë“¤ì€ ì‚¬íšŒì  ê³ ë¦½ì„ ê²½í—˜í•˜ê³ , ì´ëŠ” ì •ì‹ ì  ë° ì‹ ì²´ì  ê±´ê°•ì— ì•…ì˜...  \n",
            "2           ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ê¸€ì„ ì¨ë³´ë©´ ìì‹ ì˜ ìŠ¤íƒ€ì¼ê³¼ ê°•ì ì„ ë°œê²¬í•  ìˆ˜ ìˆë‹¤.  \n",
            "3                      ë‹¤ìš©ë„ ìˆ˜ë‚©í•¨ì€ ë‹¤ì–‘í•œ ë¬¼ê±´ì„ ì •ë¦¬í•˜ëŠ” ë° ìœ ìš©í•˜ë‹¤.  \n",
            "4     ê²°êµ­, ìŒì•…ì€ ê°œì¸ê³¼ ì§‘ë‹¨ì˜ ë¬¸í™”ì  ì •ì²´ì„±ì„ ì´í•´í•˜ëŠ” ë° í•„ìˆ˜ì ì¸ ìš”ì†Œë¡œ ì‘ìš©í•œë‹¤.  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "csv1_path = os.path.join(extract_path, 'sample_submission.csv')\n",
        "sample_submission_data = pd.read_csv(csv1_path)\n",
        "print(sample_submission_data.head())\n",
        "\n",
        "csv2_path = os.path.join(extract_path, 'train.csv')\n",
        "train_data = pd.read_csv(csv2_path)\n",
        "print(train_data.head())\n",
        "\n",
        "csv3_path = os.path.join(extract_path, 'test.csv')\n",
        "test_data = pd.read_csv(csv3_path)\n",
        "print(test_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4EP8Bg-LMct"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gap3AKJRLV09"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqwKwQ_tRc9O"
      },
      "outputs": [],
      "source": [
        "# Pairwise ìŒ ë§Œë“¤ê¸° (ì¡°í•© ì´ìš©)\n",
        "from itertools import combinations\n",
        "\n",
        "def create_pairwise_data(train_data):\n",
        "  pairs = []\n",
        "  labels = []\n",
        "  for _, row in train_data.iterrows():\n",
        "    sentences = [row['sentence_0'], row['sentence_1'], row['sentence_2'], row['sentence_3']]\n",
        "    answers = [row['answer_0'], row['answer_1'], row['answer_2'], row['answer_3']]\n",
        "\n",
        "    #ë¬¸ì¥ ìŒ ìƒì„±\n",
        "    for i, j in combinations(range(4),2):\n",
        "      pairs.append((sentences[i], sentences[j]))\n",
        "      labels.append(1 if answers[i] < answers[j] else 0)   #answersê°€ ì‘ì€ ê°’ì´ ì•ì— ì˜¤ê²Œ ì„¤ì •\n",
        "\n",
        "  return pairs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EHQe6spMZ8k"
      },
      "outputs": [],
      "source": [
        "#Dataset ì»¤ìŠ¤í…€ class ë§Œë“¤ê¸°\n",
        "class SentencePairDataset(Dataset):\n",
        "  def __init__(self, pairs, labels, tokenizer, max_length=128):\n",
        "    sentence1s = [pair[0] for pair in pairs]\n",
        "    sentence2s = [pair[1] for pair in pairs]\n",
        "\n",
        "    self.encodings = tokenizer(\n",
        "        sentence1s,\n",
        "        sentence2s,\n",
        "        add_special_tokens = True,\n",
        "        max_length = max_length,\n",
        "        padding = 'max_length',    #max_lengthë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ë”©ìœ¼ë¡œ ì±„\n",
        "        truncation=True,           #max_lengthë³´ë‹¤ ê¸¸ë©´ ìë™ìœ¼ë¡œ ìë¦„\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)    #ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•´ long typeìœ¼ë¡œ dtype\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      item = {\n",
        "        'input_ids': self.encodings['input_ids'][idx],                 #tokenizing ê²°ê³¼\n",
        "        'attention_mask': self.encodings['attention_mask'][idx],      #paddingì¸ì§€ ì•„ë‹Œì§€ êµ¬ë¶„\n",
        "        'token_type_ids': self.encodings['token_type_ids'][idx],      #ë¬¸ì¥ êµ¬ë¶„ìš© (1st ë¬¸ì¥:0, 2nd ë¬¸ì¥:1)\n",
        "        'labels': self.labels[idx]                                    #ì •ë‹µê°’\n",
        "      }\n",
        "      return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnyaPw5qOMxo"
      },
      "outputs": [],
      "source": [
        "#ë¬¸ì¥ìŒ ì´ì§„ë¶„ë¥˜ ìœ„í•œ BERT ë¶„ë¥˜ê¸° ë§Œë“¤ê¸°\n",
        "class SentencePairClassifier(nn.Module):\n",
        "  def __init__(self, model_name='klue/bert-base'):    #klue_BERT ëª¨ë¸ ì‚¬ìš©\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained(model_name)\n",
        "    self.classifier = nn.Linear(self.bert.config.hidden_size, 1)   #BERT ì¶œë ¥ ë²¡í„° ì°¨ì› = 1ë¡œ ì„¤ì •\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, token_type_ids):   #ë°ì´í„° ê³„ì‚° ê²½ë¡œ ì§€ì •\n",
        "    #BERT ëª¨ë¸ ì‚¬ìš©í•´ì„œ ë¬¸ì¥ ì„ë² ë”© ì¶”ì¶œ\n",
        "    outputs = self.bert(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask = attention_mask,\n",
        "        token_type_ids = token_type_ids\n",
        "    )\n",
        "    # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ\n",
        "    cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "    #ì´ì§„ ë¶„ë¥˜ ìˆ˜í–‰ - Sigmoid í•¨ìˆ˜ ì´ìš©\n",
        "    logits = self.classifier(cls_output)\n",
        "    return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTRC28_zQ7Le"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ í•™ìŠµ ì •ì˜\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "  model.train()\n",
        "  total_loss = 0   #ëˆ„ì  ì†ì‹¤ ê³„\n",
        "\n",
        "  for batch in tqdm(train_loader, desc='Training'):\n",
        "    #batch_data -> GPU deviceë¡œ ì´ë™ì‹œí‚¤ê¸°\n",
        "    inputs_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "    labels = batch['labels'].to(device).float()\n",
        "\n",
        "    #ìˆœì „íŒŒ - ì…ë ¥ ë°ì´í„° ëª¨ë¸ì— í†µê³¼ -> ì˜ˆì¸¡ê°’ ìƒì„±\n",
        "    logits = model(inputs_ids, attention_mask, token_type_ids)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    #ì—­ì „íŒŒ & Optimizer\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(train_loader)  #í‰ê·  ì†ì‹¤ ë°˜í™˜ (í•™ìŠµë¥  ê³„ì‚°)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTXxXTh1clu3"
      },
      "outputs": [],
      "source": [
        "# Model í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      token_type_ids = batch['token_type_ids'].to(device)\n",
        "      labels = batch['labels'].to(device).float()\n",
        "\n",
        "      logits = model(input_ids, attention_mask, token_type_ids)\n",
        "      loss = criterion(logits, labels)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      preds = (torch.sigmoid(logits)>0.5).long()\n",
        "      all_preds.extend(preds.cpu().tolist())\n",
        "      all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "  metrics = calculate_metrics(all_labels, all_preds)\n",
        "  metrics['predictions'] = all_preds\n",
        "  return total_loss / len(data_loader), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjbYwSjFAzDO"
      },
      "outputs": [],
      "source": [
        "# metrics ì •ì˜\n",
        "def calculate_metrics(labels, preds):\n",
        "  return {\n",
        "      'accuracy': accuracy_score(labels, preds),\n",
        "      'f1': f1_score(labels, preds)\n",
        "  }\n",
        "\n",
        "def print_metrics(metrics):\n",
        "  print(f'Validation metrics:')\n",
        "  print(f'Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
        "  print(f'F1-score: {metrics[\"f1\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYHwHl5dy93c"
      },
      "outputs": [],
      "source": [
        "#ì˜ˆì¸¡ ì €ì¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ìƒì„±\n",
        "def save_predictions(labels, preds, filename='predictions.csv'):\n",
        "    results_df = pd.DataFrame({\n",
        "        'true_order': labels,\n",
        "        'predicted_order': preds\n",
        "    })\n",
        "    results_df.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "e8c89210d9c548448ab0f39ea77ebcdd",
            "5c7a4e91cd8c4ed29f8657d73b9b901a",
            "58780d0df3ed4c81a7b0a722f93bed3d",
            "999bdf1205fe41298ba621e5cb611941",
            "a33b654c6b094be59d12f29804bbe690",
            "deb170aa15884a5682e1052730021974",
            "df4dc108fffe40e69e99795de7da4313",
            "ae2569141d4b4f7484c1f70f41fe4da6",
            "1198759d77e846cbbae9f705d8d7e608",
            "2ff40824fa044d0995880516d93fb294",
            "4d299a1b10e1401bbedbb31a66218ce8",
            "23ec57ab27674856abb21dedc96bd884",
            "f454719b63704fe7a4106250a44521f8",
            "d1a4e23247734770b33845ef97fc03ff",
            "44323bb7ca6a4a96ac92632398a01f71",
            "bd00ca0d7aaf4d1d9422961b3fe8b6cf",
            "74a01f896cd34948b0b4f157acde9fb1",
            "deda96d27cd74af38a745abff679df10",
            "8cadd56babf2460fbfbb719ac13f4633",
            "8eef78e3431c4d0da2026a0f845b15a7",
            "5fd14c648f284fc7919af4e70be7b1ef",
            "f6ae9edb57a043d79b2ac8e4803f13fb",
            "02da456317554f748d6f0ae3d8de82dd",
            "87a701bbb6804e9399088799f5cd37f1",
            "bc5237f791e549ca8f9c9217cea5af91",
            "53490972f61e44eea7f14dbe6661e6e3",
            "e554f59109bd44299628d21f2caa5996",
            "fa563046af0f461b94cf4af51de3ef24",
            "516a3c566f5b48c680b6c43dff72a0c6",
            "52c8829c729a4032a9f3a70588f15140",
            "6c92406cde7c4a9b9a1e5958536ab9e8",
            "ada17da23a064d9d8da99d1f330610cd",
            "6b8c91e40cc9483d9f346a56abc5a2d2",
            "7b03727bbf6447da9e95fddf150c57ba",
            "24a7e92afdbf46e7891cd7ae45c0da61",
            "3654b6f16485463ea36594c74caccfa8",
            "a3914f5f529a48cba384a0da090e1c2b",
            "bafabd7545d647e39ba72b1db4356115",
            "be765dd8890546b79e2085c92acb1138",
            "f4e7654b20c744cb8d2e1db724a39c05",
            "75e0cd323b0442d1a184c08bb932d653",
            "4625998058754616a88ef5415c4367bc",
            "1e700c910d3345248bcfcac22ee95a63",
            "863059728b4e482e9e034fb27742a819",
            "df2f142dbbae426d891b8c5b7edd539a",
            "0a458de1c9b44b86b1636600ea66f153",
            "f0790d497ec24d79baa39d2f369ff6bf",
            "bd23651de779480884a8b9901a2cbcbc",
            "943167c64d974174b84684281b6e91b6",
            "c46e12f2f3e64725ae215cfc1ffe7793",
            "929a0a3a46d4475cb660715b42b0fe8b",
            "4e07ca08ee5f4b898144df1bb57a77ae",
            "0f724b3b8f3449799e610a8dc7d0299c",
            "b1921aa704ca47c08b0b036a85ea9027",
            "75149d5c27f94be9b3fcabb73608d45a",
            "6ab116a4061c486baf2bc3524114dd8b",
            "c882be7a95214de0b1670d050d53f53e",
            "763f963d24df4e5a8d6d7bd0585309e2",
            "9cb04ddd3ebb49d7aa00507e08278c49",
            "17b93aeccb3744b8b868423fe0cd0df7",
            "bd3cc61b131d4485a6d1e921cfe7dbd6",
            "4f21b5ed33464a239d08e4982db22f89",
            "ab7ee4fb482d43a7bfdd487a50838f3f",
            "98b88a0cede6458a923fbe5037b3bf5c",
            "6480a4c339a841858a78677419ec8a3f",
            "a7e6ec6072e44fd0b2313e9df0f4c636"
          ]
        },
        "id": "WpUtRMdgwu-f",
        "outputId": "1bc36f52-1a75-49ea-9eaa-bc99513c283d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8c89210d9c548448ab0f39ea77ebcdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ec57ab27674856abb21dedc96bd884",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02da456317554f748d6f0ae3d8de82dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b03727bbf6447da9e95fddf150c57ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2f142dbbae426d891b8c5b7edd539a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ab116a4061c486baf2bc3524114dd8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|â–Œ         | 112/2205 [40:01<12:29:59, 21.50s/it]"
          ]
        }
      ],
      "source": [
        "# main í•¨ìˆ˜ ì •ì˜\n",
        "def main():\n",
        "  csv2_path = os.path.join(extract_path, 'train.csv')\n",
        "  train_data = pd.read_csv(csv2_path)\n",
        "  train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "  tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
        "  train_pairs, train_labels = create_pairwise_data(train_data)\n",
        "  val_pairs, val_labels = create_pairwise_data(val_data)\n",
        "  train_dataset = SentencePairDataset(train_pairs, train_labels, tokenizer)\n",
        "  val_dataset = SentencePairDataset(val_pairs, val_labels, tokenizer)\n",
        "\n",
        "  #epochë‹¹ ì²˜ë¦¬í•  batch ê°œìˆ˜ ì„¤ì •\n",
        "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "  #ë””ë°”ì´ìŠ¤ ì„¤ì •(GPU)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(f'Using device: {device}')\n",
        "\n",
        "  #ëª¨ë¸ ì´ˆê¸°í™”\n",
        "  model = SentencePairClassifier()\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Optimizer, Loss ftn ì„¤ì •\n",
        "  optimizer =  torch.optim.AdamW(model.parameters(), lr=2e-5)   #AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()      #Binary Cross Entropy Loss ì‚¬ìš©\n",
        "\n",
        "  num_epochs = 5    #epoch=5ë¡œ ì„¤ì •\n",
        "  best_val_f1 = 0\n",
        "  patience = 2\n",
        "  counter = 0\n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_f1s = []\n",
        "    print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    print(f'Average training loss: {train_loss:.4f}')\n",
        "\n",
        "    # ê²€ì¦\n",
        "    val_loss, metrics = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Average validation loss: {val_loss:.4f}')\n",
        "    print_metrics(metrics)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_f1s.append(metrics['f1'])\n",
        "\n",
        "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
        "    if metrics['f1'] > best_val_f1:\n",
        "        best_val_f1 = metrics['f1']\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.path')\n",
        "        print(\"âœ… Best model saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"ğŸ” No improvement. Early stopping counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "  # best modelë¡œ ìµœì¢… í‰ê°€\n",
        "  print(\"\\nFinal Evaluation:\")\n",
        "  model.load_state_dict(torch.load('best_model.path'))\n",
        "  _, final_metrics = evaluate(model, val_loader, criterion, device)\n",
        "  print_metrics(final_metrics)\n",
        "\n",
        "  save_predictions(val_labels, final_metrics['predictions'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZMnlFSQyM_o"
      },
      "outputs": [],
      "source": [
        "  # best modelë¡œ ìµœì¢… í‰ê°€\n",
        "  print(\"\\nFinal Evaluation:\")\n",
        "  model.load_state_dict(torch.load('best_model.path'))\n",
        "  _, final_metrics = evaluate(model, val_loader, criterion, device)\n",
        "  print_metrics(final_metrics)\n",
        "\n",
        "  save_predictions(val_labels, final_metrics['predictions'])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIywjxFo9-11"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(val_f1s, label='Validation F1')\n",
        "    plt.title('Validation F1 over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1-score')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1054MYiKysZe9vwArH7Zj5NhSIktjkGpL",
      "authorship_tag": "ABX9TyPsgujS+vSeHAj53DK/QRJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onewon1234/AI_DL_Project/blob/KLUE-BERT/KLUE-BERT/KlueBert_code_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkl_GWPWLGnC",
        "outputId": "b3c36835-ed23-4e36-9204-8b177dc21de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZ3B4tyLG9k",
        "outputId": "fbbf76b9-08a3-43cc-8eac-a4f51c199249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "압축 해제 완료\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "file_path = '/content/drive/MyDrive/data/open.zip'\n",
        "\n",
        "extract_path = '/content/data'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"압축 해제 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOiE1UagLIFA",
        "outputId": "37309896-e8f4-488f-8025-99e7a0a6dbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sample_submission.csv', 'test.csv', 'train.csv']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(os.listdir(extract_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6HoTp-HLLTb",
        "outputId": "e3f58a04-1888-46bc-80fe-8d540655f984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          ID  answer_0  answer_1  answer_2  answer_3\n",
            "0  TEST_0000         0         1         2         3\n",
            "1  TEST_0001         0         1         2         3\n",
            "2  TEST_0002         0         1         2         3\n",
            "3  TEST_0003         0         1         2         3\n",
            "4  TEST_0004         0         1         2         3\n",
            "           ID                                         sentence_0  \\\n",
            "0  TRAIN_0000                 블록체인 기술은 투표 과정의 투명성을 크게 향상시킬 수 있다.   \n",
            "1  TRAIN_0001  줄거리 자동 생성의 인공지능 알고리즘은 대량의 텍스트 데이터를 분석하여 핵심 정보를...   \n",
            "2  TRAIN_0002  마지막으로, 키친타올을 보관할 때는 쉽게 접근할 수 있는 곳에 두어 낭비를 방지하는...   \n",
            "3  TRAIN_0003   책의 페이지가 손상되지 않도록 수직으로 세워 두거나 평평하게 눕혀 보관하는 것이 좋다.   \n",
            "4  TRAIN_0004  인공지능 모델은 반복적인 실험을 통해 지속적으로 학습하며, 이를 통해 발견의 정확성...   \n",
            "\n",
            "                                          sentence_1  \\\n",
            "0  이러한 특성은 유권자들에게 신뢰를 제공하며, 민주적 참여를 촉진하는 데 기여할 수 있다.   \n",
            "1     결과적으로, 이러한 기술은 사용자에게 신속하고 효율적인 정보 전달을 가능하게 한다.   \n",
            "2          재사용 가능한 천이나 스펀지를 활용하면 키친타올의 필요성을 줄일 수 있다.   \n",
            "3      정기적으로 먼지를 털어내고, 곰팡이나 해충의 발생 여부를 점검하는 것이 중요하다.   \n",
            "4  인공지능은 대량의 데이터를 분석하여 숨겨진 패턴과 상관관계를 발견하는 데 강력한 도...   \n",
            "\n",
            "                                          sentence_2  \\\n",
            "0  결과적으로 블록체인 기반의 투표 시스템은 공정하고 신뢰할 수 있는 선거 환경을 조성...   \n",
            "1     생성된 줄거리는 원본 텍스트의 의미를 유지하면서도 간결하게 요약된 형태로 제공된다.   \n",
            "2               물기를 제거할 때는 가볍게 눌러주어 과도한 사용을 피할 수 있다.   \n",
            "3             종이책은 직사광선이 닿지 않는 서늘하고 건조한 장소에 보관해야 한다.   \n",
            "4  결국, 인공지능의 지원은 과학적 발견의 속도와 효율성을 혁신적으로 변화시킬 수 있는...   \n",
            "\n",
            "                                          sentence_3  answer_0  answer_1  \\\n",
            "0       각 투표는 변경 불가능한 기록으로 저장되어 조작의 가능성을 원천적으로 차단한다.         0         3   \n",
            "1  이 알고리즘은 자연어 처리 기술을 활용하여 문맥을 이해하고, 주요 사건과 등장인물을...         0         3   \n",
            "2                 키친타올을 절약하는 첫걸음은 필요한 양만큼만 사용하는 것이다.         3         2   \n",
            "3  필요할 경우, 책을 보호하기 위해 커버를 씌우거나 전용 보관함에 넣는 방법도 고려할...         2         0   \n",
            "4  이러한 분석 결과는 연구자들에게 새로운 가설을 제시하고 실험 설계를 개선하는 데 기...         1         3   \n",
            "\n",
            "   answer_2  answer_3  \n",
            "0         1         2  \n",
            "1         2         1  \n",
            "2         1         0  \n",
            "3         1         3  \n",
            "4         0         2  \n",
            "          ID                                         sentence_0  \\\n",
            "0  TEST_0000  자유 의지와 결정론은 서로 상충하는 개념으로 여겨지지만, 이 둘의 공존 가능성도 탐...   \n",
            "1  TEST_0001                  사회적 낙인은 개인의 자아 존중감에 부정적인 영향을 미친다.   \n",
            "2  TEST_0002                    글쓰기 능력을 키우기 위해서는 꾸준한 연습이 필수적이다.   \n",
            "3  TEST_0003             작은 공간에서도 효율적으로 사용할 수 있어 집안의 혼잡함을 줄여준다.   \n",
            "4  TEST_0004                  음악은 특정 문화의 가치와 전통을 반영하는 중요한 매체이다.   \n",
            "\n",
            "                                          sentence_1  \\\n",
            "0  결정론은 모든 사건이 원인과 결과의 연쇄에 의해 발생한다고 주장하며, 이는 인간의 ...   \n",
            "1  건강 불평등은 이러한 낙인으로 인해 더욱 심화되며, 특정 집단이 의료 서비스 접근에...   \n",
            "2  마지막으로, 독서를 통해 다른 작가들의 기법을 배우는 것은 창의력을 자극하는 데 도...   \n",
            "3                 정기적으로 내용을 점검하면 필요 없는 물건을 정리할 수 있다.   \n",
            "4                이러한 음악적 표현은 공동체의 소속감을 증진시키는 역할을 한다.   \n",
            "\n",
            "                                          sentence_2  \\\n",
            "0  그러나 인간의 인식과 선택 과정에서 나타나는 복잡성과 예측 불가능성은 자유 의지의 ...   \n",
            "1  결국, 사회적 낙인과 건강 불평등은 서로 연결되어 있으며, 이를 해결하기 위한 포괄...   \n",
            "2              피드백을 받는 과정은 글의 질을 향상시키는 중요한 요소로 작용한다.   \n",
            "3                  각 칸을 활용하여 카테고리별로 물건을 나누면 찾기 쉬워진다.   \n",
            "4               각 문화는 고유한 음악적 요소를 통해 정체성을 형성하고 강화한다.   \n",
            "\n",
            "                                          sentence_3  \n",
            "0  결국, 자유 의지와 결정론은 서로를 배제하기보다는, 인간 경험의 다양한 측면을 설명...  \n",
            "1  낙인으로 인해 사람들은 사회적 고립을 경험하고, 이는 정신적 및 신체적 건강에 악영...  \n",
            "2           다양한 주제에 대해 글을 써보면 자신의 스타일과 강점을 발견할 수 있다.  \n",
            "3                      다용도 수납함은 다양한 물건을 정리하는 데 유용하다.  \n",
            "4     결국, 음악은 개인과 집단의 문화적 정체성을 이해하는 데 필수적인 요소로 작용한다.  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "csv1_path = os.path.join(extract_path, 'sample_submission.csv')\n",
        "sample_submission_data = pd.read_csv(csv1_path)\n",
        "print(sample_submission_data.head())\n",
        "\n",
        "csv2_path = os.path.join(extract_path, 'train.csv')\n",
        "train_data = pd.read_csv(csv2_path)\n",
        "print(train_data.head())\n",
        "\n",
        "csv3_path = os.path.join(extract_path, 'test.csv')\n",
        "test_data = pd.read_csv(csv3_path)\n",
        "print(test_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4EP8Bg-LMct"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gap3AKJRLV09"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqwKwQ_tRc9O"
      },
      "outputs": [],
      "source": [
        "# Pairwise 쌍 만들기 (조합 이용)\n",
        "from itertools import combinations\n",
        "\n",
        "def create_pairwise_data(train_data):\n",
        "  pairs = []\n",
        "  labels = []\n",
        "  for _, row in train_data.iterrows():\n",
        "    sentences = [row['sentence_0'], row['sentence_1'], row['sentence_2'], row['sentence_3']]\n",
        "    answers = [row['answer_0'], row['answer_1'], row['answer_2'], row['answer_3']]\n",
        "\n",
        "    #문장 쌍 생성\n",
        "    for i, j in combinations(range(4),2):\n",
        "      pairs.append((sentences[i], sentences[j]))\n",
        "      labels.append(1 if answers[i] < answers[j] else 0)   #answers가 작은 값이 앞에 오게 설정\n",
        "\n",
        "  return pairs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EHQe6spMZ8k"
      },
      "outputs": [],
      "source": [
        "#Dataset 커스텀 class 만들기\n",
        "class SentencePairDataset(Dataset):\n",
        "  def __init__(self, pairs, labels, tokenizer, max_length=128):\n",
        "    sentence1s = [pair[0] for pair in pairs]\n",
        "    sentence2s = [pair[1] for pair in pairs]\n",
        "\n",
        "    self.encodings = tokenizer(\n",
        "        sentence1s,\n",
        "        sentence2s,\n",
        "        add_special_tokens = True,\n",
        "        max_length = max_length,\n",
        "        padding = 'max_length',    #max_length보다 짧으면 패딩으로 채\n",
        "        truncation=True,           #max_length보다 길면 자동으로 자름\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)    #이진 분류를 위해 long type으로 dtype\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      item = {\n",
        "        'input_ids': self.encodings['input_ids'][idx],                 #tokenizing 결과\n",
        "        'attention_mask': self.encodings['attention_mask'][idx],      #padding인지 아닌지 구분\n",
        "        'token_type_ids': self.encodings['token_type_ids'][idx],      #문장 구분용 (1st 문장:0, 2nd 문장:1)\n",
        "        'labels': self.labels[idx]                                    #정답값\n",
        "      }\n",
        "      return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnyaPw5qOMxo"
      },
      "outputs": [],
      "source": [
        "#문장쌍 이진분류 위한 BERT 분류기 만들기\n",
        "class SentencePairClassifier(nn.Module):\n",
        "  def __init__(self, model_name='klue/bert-base'):    #klue_BERT 모델 사용\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained(model_name)\n",
        "    self.classifier = nn.Linear(self.bert.config.hidden_size, 1)   #BERT 출력 벡터 차원 = 1로 설정\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, token_type_ids):   #데이터 계산 경로 지정\n",
        "    #BERT 모델 사용해서 문장 임베딩 추출\n",
        "    outputs = self.bert(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask = attention_mask,\n",
        "        token_type_ids = token_type_ids\n",
        "    )\n",
        "    # [CLS] 토큰 임베딩 추출\n",
        "    cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "    #이진 분류 수행 - Sigmoid 함수 이용\n",
        "    logits = self.classifier(cls_output)\n",
        "    return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTRC28_zQ7Le"
      },
      "outputs": [],
      "source": [
        "# 모델 학습 정의\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "  model.train()\n",
        "  total_loss = 0   #누적 손실 계\n",
        "\n",
        "  for batch in tqdm(train_loader, desc='Training'):\n",
        "    #batch_data -> GPU device로 이동시키기\n",
        "    inputs_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "    labels = batch['labels'].to(device).float()\n",
        "\n",
        "    #순전파 - 입력 데이터 모델에 통과 -> 예측값 생성\n",
        "    logits = model(inputs_ids, attention_mask, token_type_ids)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    #역전파 & Optimizer\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(train_loader)  #평균 손실 반환 (학습률 계산)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTXxXTh1clu3"
      },
      "outputs": [],
      "source": [
        "# Model 평가 함수 정의\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      token_type_ids = batch['token_type_ids'].to(device)\n",
        "      labels = batch['labels'].to(device).float()\n",
        "\n",
        "      logits = model(input_ids, attention_mask, token_type_ids)\n",
        "      loss = criterion(logits, labels)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      preds = (torch.sigmoid(logits)>0.5).long()\n",
        "      all_preds.extend(preds.cpu().tolist())\n",
        "      all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "  metrics = calculate_metrics(all_labels, all_preds)\n",
        "  metrics['predictions'] = all_preds\n",
        "  return total_loss / len(data_loader), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjbYwSjFAzDO"
      },
      "outputs": [],
      "source": [
        "# metrics 정의\n",
        "def calculate_metrics(labels, preds):\n",
        "  return {\n",
        "      'accuracy': accuracy_score(labels, preds),\n",
        "      'f1': f1_score(labels, preds)\n",
        "  }\n",
        "\n",
        "def print_metrics(metrics):\n",
        "  print(f'Validation metrics:')\n",
        "  print(f'Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
        "  print(f'F1-score: {metrics[\"f1\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYHwHl5dy93c"
      },
      "outputs": [],
      "source": [
        "#예측 저장하기 위한 함수 생성\n",
        "def save_predictions(labels, preds, filename='predictions.csv'):\n",
        "    results_df = pd.DataFrame({\n",
        "        'true_order': labels,\n",
        "        'predicted_order': preds\n",
        "    })\n",
        "    results_df.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "e8c89210d9c548448ab0f39ea77ebcdd",
            "5c7a4e91cd8c4ed29f8657d73b9b901a",
            "58780d0df3ed4c81a7b0a722f93bed3d",
            "999bdf1205fe41298ba621e5cb611941",
            "a33b654c6b094be59d12f29804bbe690",
            "deb170aa15884a5682e1052730021974",
            "df4dc108fffe40e69e99795de7da4313",
            "ae2569141d4b4f7484c1f70f41fe4da6",
            "1198759d77e846cbbae9f705d8d7e608",
            "2ff40824fa044d0995880516d93fb294",
            "4d299a1b10e1401bbedbb31a66218ce8",
            "23ec57ab27674856abb21dedc96bd884",
            "f454719b63704fe7a4106250a44521f8",
            "d1a4e23247734770b33845ef97fc03ff",
            "44323bb7ca6a4a96ac92632398a01f71",
            "bd00ca0d7aaf4d1d9422961b3fe8b6cf",
            "74a01f896cd34948b0b4f157acde9fb1",
            "deda96d27cd74af38a745abff679df10",
            "8cadd56babf2460fbfbb719ac13f4633",
            "8eef78e3431c4d0da2026a0f845b15a7",
            "5fd14c648f284fc7919af4e70be7b1ef",
            "f6ae9edb57a043d79b2ac8e4803f13fb",
            "02da456317554f748d6f0ae3d8de82dd",
            "87a701bbb6804e9399088799f5cd37f1",
            "bc5237f791e549ca8f9c9217cea5af91",
            "53490972f61e44eea7f14dbe6661e6e3",
            "e554f59109bd44299628d21f2caa5996",
            "fa563046af0f461b94cf4af51de3ef24",
            "516a3c566f5b48c680b6c43dff72a0c6",
            "52c8829c729a4032a9f3a70588f15140",
            "6c92406cde7c4a9b9a1e5958536ab9e8",
            "ada17da23a064d9d8da99d1f330610cd",
            "6b8c91e40cc9483d9f346a56abc5a2d2",
            "7b03727bbf6447da9e95fddf150c57ba",
            "24a7e92afdbf46e7891cd7ae45c0da61",
            "3654b6f16485463ea36594c74caccfa8",
            "a3914f5f529a48cba384a0da090e1c2b",
            "bafabd7545d647e39ba72b1db4356115",
            "be765dd8890546b79e2085c92acb1138",
            "f4e7654b20c744cb8d2e1db724a39c05",
            "75e0cd323b0442d1a184c08bb932d653",
            "4625998058754616a88ef5415c4367bc",
            "1e700c910d3345248bcfcac22ee95a63",
            "863059728b4e482e9e034fb27742a819",
            "df2f142dbbae426d891b8c5b7edd539a",
            "0a458de1c9b44b86b1636600ea66f153",
            "f0790d497ec24d79baa39d2f369ff6bf",
            "bd23651de779480884a8b9901a2cbcbc",
            "943167c64d974174b84684281b6e91b6",
            "c46e12f2f3e64725ae215cfc1ffe7793",
            "929a0a3a46d4475cb660715b42b0fe8b",
            "4e07ca08ee5f4b898144df1bb57a77ae",
            "0f724b3b8f3449799e610a8dc7d0299c",
            "b1921aa704ca47c08b0b036a85ea9027",
            "75149d5c27f94be9b3fcabb73608d45a",
            "6ab116a4061c486baf2bc3524114dd8b",
            "c882be7a95214de0b1670d050d53f53e",
            "763f963d24df4e5a8d6d7bd0585309e2",
            "9cb04ddd3ebb49d7aa00507e08278c49",
            "17b93aeccb3744b8b868423fe0cd0df7",
            "bd3cc61b131d4485a6d1e921cfe7dbd6",
            "4f21b5ed33464a239d08e4982db22f89",
            "ab7ee4fb482d43a7bfdd487a50838f3f",
            "98b88a0cede6458a923fbe5037b3bf5c",
            "6480a4c339a841858a78677419ec8a3f",
            "a7e6ec6072e44fd0b2313e9df0f4c636"
          ]
        },
        "id": "WpUtRMdgwu-f",
        "outputId": "1bc36f52-1a75-49ea-9eaa-bc99513c283d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8c89210d9c548448ab0f39ea77ebcdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ec57ab27674856abb21dedc96bd884",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02da456317554f748d6f0ae3d8de82dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b03727bbf6447da9e95fddf150c57ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2f142dbbae426d891b8c5b7edd539a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ab116a4061c486baf2bc3524114dd8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|▌         | 112/2205 [40:01<12:29:59, 21.50s/it]"
          ]
        }
      ],
      "source": [
        "# main 함수 정의\n",
        "def main():\n",
        "  csv2_path = os.path.join(extract_path, 'train.csv')\n",
        "  train_data = pd.read_csv(csv2_path)\n",
        "  train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "  tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
        "  train_pairs, train_labels = create_pairwise_data(train_data)\n",
        "  val_pairs, val_labels = create_pairwise_data(val_data)\n",
        "  train_dataset = SentencePairDataset(train_pairs, train_labels, tokenizer)\n",
        "  val_dataset = SentencePairDataset(val_pairs, val_labels, tokenizer)\n",
        "\n",
        "  #epoch당 처리할 batch 개수 설정\n",
        "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "  #디바이스 설정(GPU)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(f'Using device: {device}')\n",
        "\n",
        "  #모델 초기화\n",
        "  model = SentencePairClassifier()\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Optimizer, Loss ftn 설정\n",
        "  optimizer =  torch.optim.AdamW(model.parameters(), lr=2e-5)   #AdamW 옵티마이저 사용\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()      #Binary Cross Entropy Loss 사용\n",
        "\n",
        "  num_epochs = 5    #epoch=5로 설정\n",
        "  best_val_f1 = 0\n",
        "  patience = 2\n",
        "  counter = 0\n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_f1s = []\n",
        "    print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    print(f'Average training loss: {train_loss:.4f}')\n",
        "\n",
        "    # 검증\n",
        "    val_loss, metrics = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Average validation loss: {val_loss:.4f}')\n",
        "    print_metrics(metrics)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_f1s.append(metrics['f1'])\n",
        "\n",
        "    # 최고 성능 모델 저장\n",
        "    if metrics['f1'] > best_val_f1:\n",
        "        best_val_f1 = metrics['f1']\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.path')\n",
        "        print(\"✅ Best model saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"🔁 No improvement. Early stopping counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "  # best model로 최종 평가\n",
        "  print(\"\\nFinal Evaluation:\")\n",
        "  model.load_state_dict(torch.load('best_model.path'))\n",
        "  _, final_metrics = evaluate(model, val_loader, criterion, device)\n",
        "  print_metrics(final_metrics)\n",
        "\n",
        "  save_predictions(val_labels, final_metrics['predictions'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZMnlFSQyM_o"
      },
      "outputs": [],
      "source": [
        "  # best model로 최종 평가\n",
        "  print(\"\\nFinal Evaluation:\")\n",
        "  model.load_state_dict(torch.load('best_model.path'))\n",
        "  _, final_metrics = evaluate(model, val_loader, criterion, device)\n",
        "  print_metrics(final_metrics)\n",
        "\n",
        "  save_predictions(val_labels, final_metrics['predictions'])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIywjxFo9-11"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(val_f1s, label='Validation F1')\n",
        "    plt.title('Validation F1 over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1-score')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1054MYiKysZe9vwArH7Zj5NhSIktjkGpL",
      "authorship_tag": "ABX9TyPsgujS+vSeHAj53DK/QRJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}